{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание цены акции по экономическим новостям\n",
    "\n",
    "Входные данные:\n",
    "* Новости о компании \"Газпром\", начиная с 2010 года\n",
    "* Стоимость акций компании \"Газпром\" на ММВБ, начиная с 2010 года\n",
    "    * цена открытия (Open)\n",
    "    * цена закрытия (ClosingPrice)\n",
    "    * максимальная цена за день (DailyHigh)\n",
    "    * минимальная цена за день (DailyLow) \n",
    "    * объем бумаг (VolumePcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('texts.csv')\n",
    "df = df[df.text != ' '] # уберем пустые записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09.11.2017</td>\n",
       "      <td>Компания рассчитывает на решение по газовому с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08.11.2017</td>\n",
       "      <td>Как и предполагал “Ъ”, «Газпром», воспользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.11.2017</td>\n",
       "      <td>Новая редакция американских санкций ставит по...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text\n",
       "0  09.11.2017  Компания рассчитывает на решение по газовому с...\n",
       "1  08.11.2017   Как и предполагал “Ъ”, «Газпром», воспользова...\n",
       "2  01.11.2017   Новая редакция американских санкций ставит по..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-11 00:00:00   2017-12-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "pr_all = pd.read_csv('gazprom_prices.csv', sep=';')\n",
    "pr_all.columns = pr_all.columns.str.lower()\n",
    "pr_all.date = pd.to_datetime(pr_all.date, format='%d.%m.%Y')\n",
    "print( pr_all.date.min(), ' ', pr_all.date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим df с ежедневными датами в выбранном диапазоне\n",
    "date_full = pd.date_range(pr_all.date.min(), pr_all.date.max(), freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full = pd.DataFrame(date_full.sort_values(ascending = False), columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full = pd.merge(pr_full, pr_all[['date', 'closingprice']], how='left', on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Известно, что если в какой-либо день не было торгов, то используется цена закрытия предыдущего дня. Позволим себе сделать такое же приближение в этой задаче. Для этого создала таблицу, в которой есть дата каждого дня, соединила с известными данными торгов. NaNы заполним с помощью функции fillna и метода backfill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_full.closingprice.fillna(method = 'bfill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>closingprice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>132,60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>133,02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>134,00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>133,65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>133,77000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>133,02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-12-02</td>\n",
       "      <td>133,02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>133,02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>132,15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>133,55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>135,18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>133,50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>133,57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>133,57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>133,57000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date closingprice\n",
       "0  2017-12-08    132,60000\n",
       "1  2017-12-07    133,02000\n",
       "2  2017-12-06    134,00000\n",
       "3  2017-12-05    133,65000\n",
       "4  2017-12-04    133,77000\n",
       "5  2017-12-03    133,02000\n",
       "6  2017-12-02    133,02000\n",
       "7  2017-12-01    133,02000\n",
       "8  2017-11-30    132,15000\n",
       "9  2017-11-29    133,55000\n",
       "10 2017-11-28    135,18000\n",
       "11 2017-11-27    133,50000\n",
       "12 2017-11-26    133,57000\n",
       "13 2017-11-25    133,57000\n",
       "14 2017-11-24    133,57000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_full.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# описание заданя оставила для удобства работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1. Вводная\n",
    "\n",
    "Проведите предобработку текстов: если считаете нужным, выполните токенизацию, приведение к нижнему регистру, лемматизацию и/или стемминг. Ответьте на следующие вопросы:\n",
    "* Есть ли корреляция между средней длинной текста за день и ценой закрытия?\n",
    "* Есть ли корреляция между количеством упоминаний Алексея Миллера  и ценой закрытия? Учтите разные варианты написания имени.\n",
    "* Упоминаний какого газопровода в статьях больше: \n",
    "    * \"северный поток\"\n",
    "    * \"турецкий поток\"?\n",
    "* О каких санкциях пишут в статьях?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 \n",
    "\n",
    "m = pymorphy2.MorphAnalyzer() \n",
    "\n",
    "# Токенизация\n",
    "# - убрала дефисы, тк в некоторых словах опечатки, оставила латиницу, тк несущие смысл слова\n",
    "def tokenize(x):\n",
    "    article = re.compile('[А-Яа-яA-Za-z]+')    \n",
    "    return article.findall(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция удаления дублирования строк текста(пример df.text.iloc[7], df.text.iloc[100])\n",
    "def del_doubles(x):\n",
    "    s_list = []\n",
    "    sents = sent_tokenize(x)\n",
    "    for i in sents: \n",
    "        s = str(' '.join(tokenize(i)))\n",
    "        s_list.append(s)    \n",
    "    return set(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' «Газпром нефть», пробурившая первую скважину на Аяшском участке шельфа Сахалина, обнаружила крупное месторождение. Геологические запасы оценены в 255\\xa0млн тонн, извлекаемые — в 70–80\\xa0млн тонн, но эксперты осторожно замечают, что об экономической эффективности пока говорить рано. Предположительно, нефтекомпания будет искать партнера в проект, чтобы разделить риски. По словам источников “Ъ”, участком интересовалась Shell, но из-за санкций иностранцы с 2014 года на шельф России пока не идут.\\r\\n «Газпром нефть», пробурившая первую скважину на Аяшском участке шельфа Сахалина, обнаружила крупное месторождение. Геологические запасы оценены в 255\\xa0млн тонн, извлекаемые — в 70–80\\xa0млн тонн, но эксперты осторожно замечают, что об экономической эффективности пока говорить рано. Предположительно, нефтекомпания будет искать партнера в проект, чтобы разделить риски. По словам источников “Ъ”, участком интересовалась Shell, но из-за санкций иностранцы с 2014 года на шельф России пока не идут.'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 516 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text_dbl'] = df.text.apply(del_doubles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_len'] = df.text_dbl.apply(lambda x: len(str(x))) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.text_dbl.apply(lambda x: tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# уберем стоп-слова из токенов, лемм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть стоп слов получена в результате анализа лемм, токенов, биграмм. Добавлялись в процессе анализа\n",
    "mystopwords = stopwords.words('russian') + ['руб','млрд','млн','г','миллиард','рубль','сей','пора']\n",
    "\n",
    "def stop_words(x):\n",
    "    return [token for token in x if not token in mystopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['tokens_sw'] = df.tokens.apply(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokens_len_word'] = df.tokens_sw.apply(len) # длина текста в словах\n",
    "df['tokens_len_letter'] = df.tokens_sw.apply(lambda x: len(' '.join(x))) # длина текста в символах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(x):\n",
    "    text_lemma = [m.parse(word)[0].normal_form for word in x]\n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemma'] = df.tokens.apply(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemma_sw'] = df.lemma.apply(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm_len_word'] = df.lemma_sw.apply(len) # длина текста в словах\n",
    "df['lemm_len_letter'] = df.lemma_sw.apply(lambda x: len(' '.join(x))) # длина текста в символах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция находит количество упоминаний токена по паттерну\n",
    "def miller(pattern, x):\n",
    "    res = []\n",
    "    res = re.findall(pattern,x.lower())\n",
    "    return len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = ' милер милерами сделали хорошее предложение согласился миллера'\n",
    "pattern = r'мил.?ер+[a-я]{0,3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['miller'] = df.text.apply(lambda x: miller(pattern, x))\n",
    "#df.miller.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упоминание газопроводов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# северный поток\n",
    "north_pattern = r'северн+[а-я]{0,3}\\s+поток+[а-я]{0,3}'\n",
    "eng_nord_pattern = r'nord.?stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['north'] = df.text.apply(lambda x: miller(north_pattern, x))\n",
    "df.north.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['eng_nord'] = df.text.apply(lambda x: miller(eng_nord_pattern, x))\n",
    "df.eng_nord.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nord_str = df.north.sum() + df.eng_nord.sum()\n",
    "nord_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого \"северный поток\" упоминается в статьях 95 раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# турецкий поток\n",
    "turkish_pattern = r'турецк+[а-я]{0,3}\\s+поток+[а-я]{0,3}'\n",
    "eng_turkish_pattern = r'turkish.?stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['turkish'] = df.text.apply(lambda x: miller(turkish_pattern, x))\n",
    "df.turkish.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['eng_turkish'] = df.text.apply(lambda x: miller(eng_turkish_pattern, x))\n",
    "df.eng_turkish.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turk_stream = df.turkish.sum() + df.eng_turkish.sum()\n",
    "turk_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого \"турецкий поток\" употребляется в статьях 39 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Корелляция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проследим, наблюдается ли корелляция длины текста (необработанного, в леммах, токенах) и цены закрытия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_pr = pd.merge(pr_full, df, how = 'right', left_on= 'date', right_on= 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_pr.closingprice = pd.to_numeric(txt_pr.closingprice.str.replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closingprice</th>\n",
       "      <th>text_len</th>\n",
       "      <th>tokens_len_word</th>\n",
       "      <th>tokens_len_letter</th>\n",
       "      <th>lemm_len_word</th>\n",
       "      <th>lemm_len_letter</th>\n",
       "      <th>miller</th>\n",
       "      <th>north</th>\n",
       "      <th>eng_nord</th>\n",
       "      <th>turkish</th>\n",
       "      <th>eng_turkish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>closingprice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022378</td>\n",
       "      <td>-0.023119</td>\n",
       "      <td>-0.021083</td>\n",
       "      <td>-0.021582</td>\n",
       "      <td>-0.020471</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.035337</td>\n",
       "      <td>-0.010322</td>\n",
       "      <td>-0.059740</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>-0.022378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996960</td>\n",
       "      <td>0.998119</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.207409</td>\n",
       "      <td>0.088838</td>\n",
       "      <td>0.087613</td>\n",
       "      <td>0.088523</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_len_word</th>\n",
       "      <td>-0.023119</td>\n",
       "      <td>0.996960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997102</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.996813</td>\n",
       "      <td>0.209061</td>\n",
       "      <td>0.088126</td>\n",
       "      <td>0.091079</td>\n",
       "      <td>0.090625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_len_letter</th>\n",
       "      <td>-0.021083</td>\n",
       "      <td>0.998119</td>\n",
       "      <td>0.997102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996126</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.210129</td>\n",
       "      <td>0.086244</td>\n",
       "      <td>0.085834</td>\n",
       "      <td>0.085984</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemm_len_word</th>\n",
       "      <td>-0.021582</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.996126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997123</td>\n",
       "      <td>0.212704</td>\n",
       "      <td>0.088557</td>\n",
       "      <td>0.087386</td>\n",
       "      <td>0.086691</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemm_len_letter</th>\n",
       "      <td>-0.020471</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.996813</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.997123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212413</td>\n",
       "      <td>0.086609</td>\n",
       "      <td>0.083302</td>\n",
       "      <td>0.083626</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miller</th>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.207409</td>\n",
       "      <td>0.209061</td>\n",
       "      <td>0.210129</td>\n",
       "      <td>0.212704</td>\n",
       "      <td>0.212413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161801</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>-0.016198</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>0.035337</td>\n",
       "      <td>0.088838</td>\n",
       "      <td>0.088126</td>\n",
       "      <td>0.086244</td>\n",
       "      <td>0.088557</td>\n",
       "      <td>0.086609</td>\n",
       "      <td>0.161801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209681</td>\n",
       "      <td>0.076162</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng_nord</th>\n",
       "      <td>-0.010322</td>\n",
       "      <td>0.087613</td>\n",
       "      <td>0.091079</td>\n",
       "      <td>0.085834</td>\n",
       "      <td>0.087386</td>\n",
       "      <td>0.083302</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.209681</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.137084</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turkish</th>\n",
       "      <td>-0.059740</td>\n",
       "      <td>0.088523</td>\n",
       "      <td>0.090625</td>\n",
       "      <td>0.085984</td>\n",
       "      <td>0.086691</td>\n",
       "      <td>0.083626</td>\n",
       "      <td>-0.016198</td>\n",
       "      <td>0.076162</td>\n",
       "      <td>0.137084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng_turkish</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   closingprice  text_len  tokens_len_word  tokens_len_letter  \\\n",
       "closingprice           1.000000 -0.022378        -0.023119          -0.021083   \n",
       "text_len              -0.022378  1.000000         0.996960           0.998119   \n",
       "tokens_len_word       -0.023119  0.996960         1.000000           0.997102   \n",
       "tokens_len_letter     -0.021083  0.998119         0.997102           1.000000   \n",
       "lemm_len_word         -0.021582  0.996817         0.999071           0.996126   \n",
       "lemm_len_letter       -0.020471  0.998147         0.996813           0.999328   \n",
       "miller                 0.010802  0.207409         0.209061           0.210129   \n",
       "north                  0.035337  0.088838         0.088126           0.086244   \n",
       "eng_nord              -0.010322  0.087613         0.091079           0.085834   \n",
       "turkish               -0.059740  0.088523         0.090625           0.085984   \n",
       "eng_turkish                 NaN       NaN              NaN                NaN   \n",
       "\n",
       "                   lemm_len_word  lemm_len_letter    miller     north  \\\n",
       "closingprice           -0.021582        -0.020471  0.010802  0.035337   \n",
       "text_len                0.996817         0.998147  0.207409  0.088838   \n",
       "tokens_len_word         0.999071         0.996813  0.209061  0.088126   \n",
       "tokens_len_letter       0.996126         0.999328  0.210129  0.086244   \n",
       "lemm_len_word           1.000000         0.997123  0.212704  0.088557   \n",
       "lemm_len_letter         0.997123         1.000000  0.212413  0.086609   \n",
       "miller                  0.212704         0.212413  1.000000  0.161801   \n",
       "north                   0.088557         0.086609  0.161801  1.000000   \n",
       "eng_nord                0.087386         0.083302  0.053370  0.209681   \n",
       "turkish                 0.086691         0.083626 -0.016198  0.076162   \n",
       "eng_turkish                  NaN              NaN       NaN       NaN   \n",
       "\n",
       "                   eng_nord   turkish  eng_turkish  \n",
       "closingprice      -0.010322 -0.059740          NaN  \n",
       "text_len           0.087613  0.088523          NaN  \n",
       "tokens_len_word    0.091079  0.090625          NaN  \n",
       "tokens_len_letter  0.085834  0.085984          NaN  \n",
       "lemm_len_word      0.087386  0.086691          NaN  \n",
       "lemm_len_letter    0.083302  0.083626          NaN  \n",
       "miller             0.053370 -0.016198          NaN  \n",
       "north              0.209681  0.076162          NaN  \n",
       "eng_nord           1.000000  0.137084          NaN  \n",
       "turkish            0.137084  1.000000          NaN  \n",
       "eng_turkish             NaN       NaN          NaN  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_pr.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корелляция между длиной текста и ценой закрытия не наблюдается. Оцениваем по первой строке и столбцам с суффиксом '_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# выделим отдельно DF, в котором будут данные о цене закрытия и числом умпоминайний имени \"Алексей Миллер\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_miller = txt_pr[['closingprice', 'miller']][txt_pr.miller > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 102 entries, 40 to 1200\n",
      "Data columns (total 2 columns):\n",
      "closingprice    102 non-null float64\n",
      "miller          102 non-null int64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 2.4 KB\n"
     ]
    }
   ],
   "source": [
    "df_miller.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closingprice</th>\n",
       "      <th>miller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>closingprice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miller</th>\n",
       "      <td>0.056183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              closingprice    miller\n",
       "closingprice      1.000000  0.056183\n",
       "miller            0.056183  1.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miller.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корелляция между числом упоминаний 'Алексей Миллер' и ценой закрытия не наблюдается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упоминание санкций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sanction = r'санкц+[а-я]{0,5}\\s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sanct'] = df.text.apply(lambda x: miller(sanction, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sanct.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanct = df[df.sanct > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanct.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_sanct['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "N_best = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_for_sanct = []\n",
    "for i in range(len(df_sanct)):\n",
    "    tokens_for_sanct.extend(df_sanct.lemma_sw.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем связные с санкциями биграмы и триграмы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() # класс для мер ассоциации биграм\n",
    "finder = BigramCollocationFinder.from_words(tokens_for_sanct) # класс для хранения и извлечения биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(3) # избавимся от биграм, которые встречаются реже трех раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_freq_ranking = [' '.join(i) for i in finder.nbest(bigram_measures.raw_freq, N_best)] # выбираем топ-10 биграм по частоте \n",
    "tscore_ranking = [' '.join(i) for i in finder.nbest(bigram_measures.student_t, N_best)] # выбираем топ-100 бигра по каждой мере \n",
    "pmi_ranking =  [' '.join(i) for i in finder.nbest(bigram_measures.pmi, N_best)]\n",
    "llr_ranking = [' '.join(i) for i in finder.nbest(bigram_measures.likelihood_ratio, N_best)]\n",
    "chi2_ranking =  [' '.join(i) for i in finder.nbest(bigram_measures.chi_sq, N_best)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_freq</th>\n",
       "      <th>pmi</th>\n",
       "      <th>t_score</th>\n",
       "      <th>chi2</th>\n",
       "      <th>llr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>газпром нефть</td>\n",
       "      <td>ангнуть голландский</td>\n",
       "      <td>газпром нефть</td>\n",
       "      <td>ангнуть голландский</td>\n",
       "      <td>газпром нефть</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>российский компания</td>\n",
       "      <td>чёрный список</td>\n",
       "      <td>российский компания</td>\n",
       "      <td>топ менеджер</td>\n",
       "      <td>топ менеджер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>данные ъ</td>\n",
       "      <td>nord stream</td>\n",
       "      <td>данные ъ</td>\n",
       "      <td>чистый прибыль</td>\n",
       "      <td>первое полугодие</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>алексей миллер</td>\n",
       "      <td>кредитный линия</td>\n",
       "      <td>топ менеджер</td>\n",
       "      <td>чёрный список</td>\n",
       "      <td>алексей миллер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>газа год</td>\n",
       "      <td>чистый прибыль</td>\n",
       "      <td>первое полугодие</td>\n",
       "      <td>первое полугодие</td>\n",
       "      <td>данные ъ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              raw_freq                  pmi              t_score  \\\n",
       "0        газпром нефть  ангнуть голландский        газпром нефть   \n",
       "1  российский компания        чёрный список  российский компания   \n",
       "2             данные ъ          nord stream             данные ъ   \n",
       "3       алексей миллер      кредитный линия         топ менеджер   \n",
       "4             газа год       чистый прибыль     первое полугодие   \n",
       "\n",
       "                  chi2               llr  \n",
       "0  ангнуть голландский     газпром нефть  \n",
       "1         топ менеджер      топ менеджер  \n",
       "2       чистый прибыль  первое полугодие  \n",
       "3        чёрный список    алексей миллер  \n",
       "4     первое полугодие          данные ъ  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings = pd.DataFrame({ 'chi2': chi2_ranking, 'llr':llr_ranking, 't_score' : tscore_ranking, 'pmi': pmi_ranking, 'raw_freq':raw_freq_ranking})\n",
    "rankings = rankings[['raw_freq', 'pmi', 't_score', 'chi2', 'llr']]\n",
    "rankings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    западный санкция\n",
       "21        санкция мочь\n",
       "22      санкция против\n",
       "23         санкция сша\n",
       "55    штрафной санкция\n",
       "Name: raw_freq, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.raw_freq[rankings.raw_freq.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    штрафной санкция\n",
       "31      санкция против\n",
       "40         санкция сша\n",
       "43    западный санкция\n",
       "52        санкция мочь\n",
       "Name: pmi, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.pmi[rankings.pmi.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19      санкция против\n",
       "20         санкция сша\n",
       "21    западный санкция\n",
       "24        санкция мочь\n",
       "43    штрафной санкция\n",
       "Name: t_score, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.t_score[rankings.t_score.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29      санкция против\n",
       "32    штрафной санкция\n",
       "39         санкция сша\n",
       "44    западный санкция\n",
       "52        санкция мочь\n",
       "Name: chi2, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.chi2[rankings.chi2.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26      санкция против\n",
       "34    штрафной санкция\n",
       "37         санкция сша\n",
       "43    западный санкция\n",
       "52        санкция мочь\n",
       "Name: llr, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.llr[rankings.llr.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, санкции упоминаются в контексте: штрафные, западные, санкции; санкции сша; санкции против (газпрома *мое примечание)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_best = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures # класс для мер ассоциации триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_finder = TrigramCollocationFinder.from_words(tokens_for_sanct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_freq_ranking_tr = [' '.join(i) for i in trigram_finder.nbest(trigram_measures.raw_freq, N_best)] # выбираем топ-100 биграм по частоте \n",
    "tscore_ranking_tr = [' '.join(i) for i in trigram_finder.nbest(trigram_measures.student_t, N_best)] # выбираем топ-100 биграм по каждой мере \n",
    "pmi_ranking_tr =  [' '.join(i) for i in trigram_finder.nbest(trigram_measures.pmi, N_best)]\n",
    "llr_ranking_tr = [' '.join(i) for i in trigram_finder.nbest(trigram_measures.likelihood_ratio, N_best)]\n",
    "chi2_ranking_tr =  [' '.join(i) for i in trigram_finder.nbest(trigram_measures.chi_sq, N_best)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_freq</th>\n",
       "      <th>pmi</th>\n",
       "      <th>t_score</th>\n",
       "      <th>chi2</th>\n",
       "      <th>llr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>стать известно ъ</td>\n",
       "      <td>central asia ag</td>\n",
       "      <td>стать известно ъ</td>\n",
       "      <td>central asia ag</td>\n",
       "      <td>газпром нефть планировать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ангнуть голландский shell</td>\n",
       "      <td>ictv понедельник вечером</td>\n",
       "      <td>ангнуть голландский shell</td>\n",
       "      <td>ictv понедельник вечером</td>\n",
       "      <td>лукойл газпром нефть</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>западный рынок капитал</td>\n",
       "      <td>naftna industrija srbija</td>\n",
       "      <td>западный рынок капитал</td>\n",
       "      <td>naftna industrija srbija</td>\n",
       "      <td>shell газпром нефть</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    raw_freq                       pmi  \\\n",
       "0           стать известно ъ           central asia ag   \n",
       "1  ангнуть голландский shell  ictv понедельник вечером   \n",
       "2     западный рынок капитал  naftna industrija srbija   \n",
       "\n",
       "                     t_score                      chi2  \\\n",
       "0           стать известно ъ           central asia ag   \n",
       "1  ангнуть голландский shell  ictv понедельник вечером   \n",
       "2     западный рынок капитал  naftna industrija srbija   \n",
       "\n",
       "                         llr  \n",
       "0  газпром нефть планировать  \n",
       "1       лукойл газпром нефть  \n",
       "2        shell газпром нефть  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings_tr = pd.DataFrame({ 'chi2': chi2_ranking_tr, 'llr':llr_ranking_tr, 't_score' : tscore_ranking_tr, 'pmi': pmi_ranking_tr, 'raw_freq':raw_freq_ranking_tr})\n",
    "rankings_tr = rankings_tr[['raw_freq', 'pmi', 't_score', 'chi2', 'llr']]\n",
    "rankings_tr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32                      санкция сша ес\n",
       "88            shell санкция иностранец\n",
       "152       американский санкция ставить\n",
       "153            анализ ситуация санкция\n",
       "196           банка санкция территория\n",
       "223    введение санкция предоставление\n",
       "224            введение санкция против\n",
       "228            ввести санкция компания\n",
       "255         визовый финансовый санкция\n",
       "258              виток санкция сторона\n",
       "274            влияние санкция считать\n",
       "275         влияние санкция финансовый\n",
       "287      возможный последствие санкция\n",
       "294             волна санкция объявить\n",
       "295      волна санкция проигнорировать\n",
       "299           вопрос поскольку санкция\n",
       "Name: raw_freq, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings_tr.raw_freq[rankings_tr.raw_freq.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34    санкция сша ес\n",
       "Name: t_score, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings_tr.t_score[rankings_tr.t_score.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103         западный санкция против\n",
       "185         введение санкция против\n",
       "233    существенный влияние санкция\n",
       "249       институт развитие санкция\n",
       "274             попасть санкция сша\n",
       "283                  санкция сша ес\n",
       "Name: llr, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings_tr.llr[rankings_tr.llr.str.contains('санкция') == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Триграммы дают большую ясность об упоминании санкций. Это: санкции сша и ес; западные, американские, шрафные, визовые и финансовые. Упоминается волна введенных санкций, рассматривается их влияние и последствия. Введение санкций к газовым корпорациям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные по t_score и llr не дали информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# задание оставлено для удобства работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Классификационная\n",
    "Вам предстоит решить следующую задачу: по текстам новостей за день определить, вырастет или понизится цена закрытия.\n",
    "Для этого:\n",
    "* бинаризуйте признак \"цена закрытия\":  новый признак ClosingPrice_bin равен 1, если по сравнению со вчера цена не упала, и 0 – в обратном случаея;\n",
    "* составьте бучающее и тестовое множество: данные до начала 2016 года используются для обучения, данные с 2016 года и позже – для тестирования.\n",
    "\n",
    "Таким образом, в каждлый момент времени мы знаем: \n",
    "* ClosingPrice_bin – бинарый целевой признак\n",
    "* слова из статей, опубликованных в этот день – объясняющие признаки\n",
    "\n",
    "В этой части задания вам нужно сделать baseline алгоритм и попытаться его улучшить в следующей части. \n",
    "\n",
    "Используйте любой известный вам алгоритм классификации текстов для того, Используйте $tf-idf$ преобразование, сингулярное разложение, нормировку признакого пространства и любые другие техники обработки данных, которые вы считаете нужным. Используйте accuracy и F-measure для оценки качества классификации. Покажите, как  $tf-idf$ преобразование или сингулярное разложение или любая другая использованная вами техника влияет на качество классификации.\n",
    "Если у выбранного вами алгоритма есть гиперпараметры (например, $\\alpha$ в преобразовании Лапласа для метода наивного Байеса), покажите, как изменение гиперпараметра влияет на качество классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_pr['closingprice_bin'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e68399b8774412907852de8ea0cc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# бинаризируем\n",
    "for i in tqdm_notebook( range(1201, 0, -1)):\n",
    "    #print(i ,   txt_pr.closingprice.iloc[i-1],txt_pr.closingprice.iloc[i] )\n",
    "    if txt_pr.closingprice.iloc[i-1] > txt_pr.closingprice.iloc[i]:\n",
    "        txt_pr.closingprice_bin.iloc[i-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>closingprice</th>\n",
       "      <th>closingprice_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>131.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>132.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>126.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-30</td>\n",
       "      <td>125.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>126.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>126.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  closingprice  closingprice_bin\n",
       "0 2017-11-09        131.50                 0\n",
       "1 2017-11-08        132.30                 1\n",
       "2 2017-11-01        126.50                 1\n",
       "3 2017-10-30        125.98                 0\n",
       "4 2017-10-23        126.80                 1\n",
       "5 2017-10-20        126.70                 0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_pr[['date','closingprice','closingprice_bin']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = txt_pr[txt_pr.date < pd.to_datetime('2016-1-1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = txt_pr[txt_pr.date > pd.to_datetime('2015-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text(x):\n",
    "    a = ' '.join(x)\n",
    "    #print(a)\n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# введем числовые признаки и создадим классификаторы на основе\n",
    "# CountVectorizer, tf-idf с логистическоей регрессией, наимвным байесом, подберем лучшие параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# разделим на тестовую и обучающие выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train[['closingprice_bin','closingprice','lemm_len_word','lemma_sw','tokens_sw','text_dbl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain = train['closingprice_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest = test[['closingprice_bin','closingprice','lemm_len_word','lemma_sw','tokens_sw','text_dbl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytest = test['closingprice_bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "для классификаторов MultinomialNB(), LogisticRegression() подберем лучшие параметры, \n",
    "методов обработки текста и извлечению параметров и выберем наилучшее сочетание для классификации текста простейшими средствами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cv_nb = Pipeline([('vect', CountVectorizer() ),\n",
    "                       ('tfidf', TfidfTransformer() ),\n",
    "                       ('clf', MultinomialNB() )])\n",
    "param_cv_nb = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),  # unigrams or bigrams and n-grams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.3, 0.5, 1.0, 10.0, 20.0,  50.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_cv_nb, param_cv_nb, n_jobs=-1, verbose=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': (0.3, 0.5, 1.0, 10.0, 20.0, 50.0)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(Xtrain.tokens_sw.apply(text), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53418803418803418"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "\n",
    "# text_dbl 0.53311965811965811\n",
    "# tokens_sw 0.53418803418803418\n",
    "# lemma_sw 0.53632478632478631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшая точность классификации для лемм без стоп-слов, хотя точность классификации низкая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cv_lr = Pipeline([('vect', CountVectorizer() ), \n",
    "                       ('tfidf', TfidfTransformer() ), \n",
    "                       ('clf', LogisticRegression() )])\n",
    "\n",
    "param_cv_lr = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),  # unigrams or bigrams and n-grams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__penalty': ('l1','l2'),\n",
    "    'clf__C' : (0.001, 0.01, 0.1, 1, 2, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_cv_lr, param_cv_lr, n_jobs=-1, verbose=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__penalty': ('l1', 'l2'), 'clf__C': (0.001, 0.01, 0.1, 1, 2, 5)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(Xtrain.text_dbl.apply(text), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56196581196581197"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "\n",
    "# text_dbl 0.56196581196581197\n",
    "# tokens_sw 0.53525641025641024\n",
    "# lemma_sw 0.54059829059829057"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшая точность классификации для текста без лемматизации и токенизации, хотя точность классификации низкая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_tdf_nb = Pipeline([('vect', TfidfVectorizer() ), \n",
    "                        ('clf', MultinomialNB() )])\n",
    "\n",
    "param_tdf_nb = {\n",
    "    'vect__max_df': (0.3, 0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),  # unigrams or bigrams and n-grams\n",
    "    'clf__alpha': (0.3, 0.5, 0.7, 0.9, 1.0, 10.0, 20.0,  50.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_tdf_nb, param_tdf_nb, n_jobs=-1, verbose=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.3, 0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), 'clf__alpha': (0.3, 0.5, 0.7, 0.9, 1.0, 10.0, 20.0, 50.0)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(Xtrain.lemma_sw.apply(text), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53418803418803418"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "\n",
    "# text_dbl 0.53205128205128205\n",
    "# tokens_sw 0.53205128205128205\n",
    "# lemma_sw 0.53311965811965811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшая точность классификации для текста c лемматизацией, хотя точность классификации низкая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_tdf_lr = Pipeline([('vect', TfidfVectorizer() ), \n",
    "                        ('clf', LogisticRegression() )])\n",
    "\n",
    "param_tdf_lr = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),  # unigrams or bigrams and n-grams\n",
    "    'clf__penalty': ('l1','l2'),\n",
    "    'clf__C' : (0.001, 0.01, 0.1, 1, 2, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_tdf_lr, param_tdf_lr, n_jobs=-1, verbose=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), 'clf__penalty': ('l1', 'l2'), 'clf__C': (0.001, 0.01, 0.1, 1, 2, 5)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(Xtrain.lemma_sw.apply(text), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53846153846153844"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "\n",
    "# text_dbl 0.56196581196581197\n",
    "# tokens_sw 0.53205128205128205\n",
    "# lemma_sw 0.53525641025641024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее высокий результат классификации получен логистической регрессией, используем его. Характеристики извлечем CountVectorizer(). Используем лучшие параметры, которые дал gridsearch. n-граммы с числом слов больше 3 ухудшают качество предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1,\n",
       " 'clf__penalty': 'l1',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvt = CountVectorizer(max_df = 1.0, ngram_range = (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = cvt.fit_transform(Xtrain.text_dbl.apply(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cvt.transform(Xtest.text_dbl.apply(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf = TfidfTransformer(use_idf=False, norm= 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = tfdf.fit_transform(X_train)\n",
    "X_test = tfdf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266, 14587)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(penalty='l1', C = 1.0).fit(X_train, ytrain)\n",
    "predicted = clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.5489\n",
      "f1=0.3258\n",
      "micro F1=0.5489\n",
      "macro F1=0.4934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(predicted, ytest)\n",
    "f1 = f1_score(predicted, ytest)\n",
    "micro_f1 = f1_score(predicted, ytest, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, ytest, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('f1={0:1.4f}'. format(f1))\n",
    "print('micro F1={0:1.4f}'.format(micro_f1))\n",
    "print('macro F1={0:1.4f}\\n'.format(macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEICAYAAACpqsStAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLkkFM+KSVLD/B6HfAe85aStNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee41bd1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(predicted,ytest), annot=True, fmt=\"d\", cbar=False, xticklabels=['down','up'], yticklabels=['down','up'])\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нельзя сказать, что полученный классификатор хорошо предсказывает падение и повышение цены. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Творческая\n",
    "Придумайте и попытайтесь сделать еще что-нибудь, чтобы улучшить качество классификации. \n",
    "Направления развития:\n",
    "* Морфологический признаки: \n",
    "    * использовать в качестве признаков только существительные или только именованные сущности;\n",
    "* Модели скрытых тем:\n",
    "    * использовать в качестве признаков скрытые темы;\n",
    "    * использовать в качестве признаков динамические скрытые темы \n",
    "    пример тут: (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb)\n",
    "* Синтаксические признаки:\n",
    "    * использовать SOV-тройки в качестве признаков\n",
    "    * кластеризовать SOV-тройки по эмбеддингам глаголов (обученные word2vec модели можно скачать отсюда: (http://rusvectores.org/ru/models/) и использовать только центроиды кластеров в качестве признаков\n",
    "* что-нибудь еще     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# используем только существительные и скрытые темы вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поиск существительных в тексте\n",
    "def nouns(x):\n",
    "    noun = set()\n",
    "    for w in x:\n",
    "        p = m.parse(w)[0].tag\n",
    "        if 'NOUN' in p:\n",
    "            noun.add(w)\n",
    "    return noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# найдем существительные в леммах и токенах, будем использовать их как признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "Xtrain['lemma_noun'] = Xtrain.lemma_sw.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "Xtrain['tokens_noun'] = Xtrain.tokens_sw.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "Xtest['lemma_noun'] = Xtest.lemma_sw.apply(nouns)\n",
    "Xtest['tokens_noun'] = Xtest.tokens_sw.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем tf-idf и наивный байес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tdf_nb = Pipeline([('vect', TfidfVectorizer() ), \n",
    "                        ('clf', MultinomialNB() )])\n",
    "\n",
    "param_tdf_nb = {\n",
    "    'vect__max_df': (0.3, 0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),  # unigrams or bigrams and n-grams\n",
    "    'clf__alpha': (0.3, 0.5, 0.7, 0.9, 1.0, 10.0, 20.0,  50.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_tdf_nb, param_tdf_nb, n_jobs=-1, verbose=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверим, для каких существительных (токены, леммы) классификатор дает лучший результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:   40.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.3, 0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), 'clf__alpha': (0.3, 0.5, 0.7, 0.9, 1.0, 10.0, 20.0, 50.0)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(Xtrain.lemma_noun.apply(text), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54700854700854706"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "# accuracy\n",
    "# tokens_noun 0.53311965811965811\n",
    "# lemma_noun 0.54700854700854706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделение только существительных не дало прироста качества классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
